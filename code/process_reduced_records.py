"""Raw corpora and/or statistics files generation from reducedListRecords

@author Christoph Broschinski (https://github.com/cbroschinski)

This script works on the reduced ListRecords files generated by 
create_reduced_records.py
Two operating modes are available: Raw corpus generation (-C) and 
statistic files generation (-S), combining them in one run is possible.
The rules for corpus generation are controlled via command line
parameters (-h provides an overview) 
To use the same settings as described in the master thesis, one should
call the script with the additional arguments -a (add additional
DDC classes found in the 'subject' field) and -r (polyglot uses only
reliable language classifications)

In -S mode, statistic files (corresponding to each reducedListRecord)
will be written to STATS_DIR (usually "data/stats").

In -C mode, "de" and "en" directories will be created inside CORPUS_DIR
to hold the language-specific raw corpus files, so the default paths are:

English raw corpus: data/corpus/en
German raw corpus: data/corpus/de

"""

import argparse
import json
import os
import re
import sys

from copy import deepcopy
from math import inf
import multiprocessing as mp
from time import sleep

from polyglot.detect import Detector
import pycld2

PROCESS_POOL = []
CONTENT_WAITING_QUEUE = []
MAX_PROCESSES = 8

DDC_VOCAB = {}

DDC_VOCAB_FILE = "en_ddc.tsv"
RLR_DIR = "../data/reducedListRecords"
STATS_DIR = "../data/stats"
CORPUS_DIR = "../data/corpus"

SUBJECT_DDC_REGEX = re.compile(r"\s*(ddc:|info:eu-repo/classification/ddc/)(?P<ddc>\d\d\d)\s*", re.IGNORECASE)
DDC_REGEX = re.compile(r"\s*(?P<ddc>\d\d\d)\s*")

class Stats(object):

    STATS_TEMPLATE = {
        "languages": {
            "desc_min_length": {
                "reliable": {},
                "all": {}
            },
             "not_desc_min_length": {
                "reliable": {},
                "all": {}
            },
        },
        "descriptions": {
            "num_descs_per_record": {},
            "combined_desc_lengths": {} # bins with a size of 10
        },
        "ddc_data": {
            "classcodes": {
                "num_codes_per_record": {},
                "codes": {}
            },
            "subject_classcodes": {
                "num_codes_per_record": {},
                "codes": {}
            },
            "standalone_subject_classcodes": { # Records with code information only in subject_classcodes
                "num_codes_per_record": {},
                "codes": {}
            },
            "combined_classcodes": { # Deduplicated list of classcodes and subject_classcodes
                "num_codes_per_record": {},
                "codes": {}
            },
            "auto_classcodes": {
                "num_codes_per_record": {},
                "codes": {}
            },
            "both_codes": { # Records with both autoclasscode and (subject_)classcode 
                "codes": {}
            }
        },
        "corpus": {
            "de": {
                "count": 0,
                "classcodes": {}
            },
            "en": {
                "count": 0,
                "classcodes": {}
            }
        },
        "processing_stats": { # reasons for records not making it into the corpus, in contrast to other stats this depends on processing order.
            "min_length": 0,
            "no_classcodes": 0,
            "lang_detection_failure": 0,
            "lang_detection_unreliable": 0,
            "lang_min_confidence": 0,
            "other_lang": 0,
            "eligible": 0
        }
    }

    def __init__(self, file_number, stats_dir):
        self.file_number = file_number
        self.stats_dir = stats_dir
        self.stats = deepcopy(self.STATS_TEMPLATE)

    def create_corpus_stats(self, lang, codes, desc):
        length = len(desc)
        code_combo = ":".join(codes)
        self.stats["corpus"][lang]["count"] += 1
        if code_combo not in self.stats["corpus"][lang]["classcodes"]:
            self.stats["corpus"][lang]["classcodes"][code_combo] = [length]
        else:
            self.stats["corpus"][lang]["classcodes"][code_combo].append(length)

    def create_desc_stats(self, descriptions):
        count = len(descriptions)
        if count not in self.stats["descriptions"]["num_descs_per_record"]:
            self.stats["descriptions"]["num_descs_per_record"][count] = 1
        else:
            self.stats["descriptions"]["num_descs_per_record"][count] += 1

        desc_combined = " ".join(descriptions)
        if len(desc_combined) == 0:
            key = "0"
        elif len(desc_combined) < 10:
            key = "1-9"
        else:
            lower_limit = len(desc_combined) - (len(desc_combined) % 10)
            upper_limit = lower_limit + 9
            key = str(lower_limit) + "-" + str(upper_limit)
        if key not in self.stats["descriptions"]["combined_desc_lengths"]:
            self.stats["descriptions"]["combined_desc_lengths"][key] = 1
        else:
            self.stats["descriptions"]["combined_desc_lengths"][key] += 1

    def create_classcode_stats(self, classcodes, subject_classcodes, auto_classcodes):
        code_data = {
            "classcodes": classcodes,
            "subject_classcodes": subject_classcodes,
            "auto_classcodes": auto_classcodes,
            "combined_classcodes": list(set(classcodes + subject_classcodes))
        }
        if len(subject_classcodes) > 0 and len(classcodes) == 0:
            code_data["standalone_subject_classcodes"] = subject_classcodes
        for code_type, code_data in code_data.items():
            num_codes = str(len(code_data))
            if num_codes not in self.stats["ddc_data"][code_type]["num_codes_per_record"]:
                self.stats["ddc_data"][code_type]["num_codes_per_record"][num_codes] = 1
            else:
                self.stats["ddc_data"][code_type]["num_codes_per_record"][num_codes] += 1
            if code_data:
                code_combo = ":".join(code_data)
                if code_combo not in self.stats["ddc_data"][code_type]["codes"]:
                    self.stats["ddc_data"][code_type]["codes"][code_combo] = 1
                else:
                    self.stats["ddc_data"][code_type]["codes"][code_combo] += 1
        if len(auto_classcodes) > 0:
            classcodes_combined = list(set(classcodes + subject_classcodes))
            if len(classcodes_combined) > 0:
                combo_key =  ":".join(classcodes_combined) + "<->" + ":".join(auto_classcodes)
                if combo_key not in self.stats["ddc_data"]["both_codes"]["codes"]:
                    self.stats["ddc_data"]["both_codes"]["codes"][combo_key] = 1
                else:
                    self.stats["ddc_data"]["both_codes"]["codes"][combo_key] += 1

    def create_language_stats(self, detector, args, description_combined):
        result = {
            "all": None,
            "reliable": None
        }
        if len(description_combined) == 0:
            for key in result.keys():
                result[key] = "description_empty"
        elif detector is None:
            for key in result.keys():
                result[key] = "detection_failure"
        elif detector.language.confidence < args.language_min_confidence:
            for key in result.keys():
                result[key] = "confidence_too_low"
        else:
            result["all"] = detector.language.name
            if detector.reliable:
                result["reliable"] = detector.language.name
            else:
                result["reliable"] = "unreliable"
        desc_type = "desc_min_length"
        if len(description_combined) < args.desc_min_length:
            desc_type ="not_desc_min_length"
        for category, lang in result.items():
            if lang not in self.stats["languages"][desc_type][category]:
                self.stats["languages"][desc_type][category][lang] = 1
            else:
                self.stats["languages"][desc_type][category][lang] += 1

    def write_stats_file(self):
        file_name = "stats." + self.file_number
        with open(os.path.join(self.stats_dir, file_name), "w") as f:
            f.write(json.dumps(self.stats, indent=2, sort_keys=True, ensure_ascii=False))

def extract_subject_classcodes(subjects):
    ret = []
    for subject in subjects:
        match = SUBJECT_DDC_REGEX.match(subject)
        if match:
            ret.append(match.group("ddc"))
    if len(ret) > 1:
        ret.sort()
    return ret

def extract_classcodes(classcodes, file_number):
    ret = []
    for code in classcodes:
        match = DDC_REGEX.match(code)
        if match:
            ret.append(match.group("ddc"))
        else:
            print("invalid classcode in {}: {}".format(file_number, code))
    if len(ret) > 1:
        ret.sort()
    return ret

def process_content(content, file_number, args):
    corpus_candidates = {
        "de": [],
        "en": [],
    }
    stats = Stats(file_number, STATS_DIR)
    for record in content:
        record_eligible = True
        # if no stats are requested, we can speed up the process by
        # continuing early
        try:
            description_combined = " ".join(record["description"])
        except KeyError:
            print(record)
            sys.exit()
        if len(description_combined) < args.desc_min_length:
            if record_eligible:
                stats.stats["processing_stats"]["min_length"] += 1
                record_eligible = False
            if not args.stats:
                continue
        stats.create_desc_stats(record["description"])
        classcodes = extract_classcodes(record["classcode"], file_number)
        subject_classcodes = []
        if args.additional_ddc_sources:
            subject_classcodes = extract_subject_classcodes(record["subject"])
        if not classcodes and not subject_classcodes:
            if record_eligible:
                stats.stats["processing_stats"]["no_classcodes"] += 1
                record_eligible = False
            if not args.stats:
                continue
        auto_classcodes = record["autoclasscode"]
        if len(auto_classcodes) > 1:
            auto_classcodes.sort()
        stats.create_classcode_stats(classcodes, subject_classcodes, auto_classcodes)
        det = None
        if len(description_combined) > 0:
            try:
                det = Detector(description_combined, quiet=True)
            except pycld2.error:
                # The underlying pycld2 lib may fail if the input contains
                # malformed utf-8 bytes. We treat these cases as "detection failure"
                pass
        if det is None:
            if record_eligible:
                stats.stats["processing_stats"]["lang_detection_failure"] += 1
                record_eligible = False
            if not args.stats:
                continue
        if det and args.reliable_predictions_only and not det.reliable:
            if record_eligible:
                stats.stats["processing_stats"]["lang_detection_unreliable"] += 1
                record_eligible = False
            if not args.stats:
                continue
        if det and det.language.confidence < args.language_min_confidence:
            if record_eligible:
                stats.stats["processing_stats"]["lang_min_confidence"] += 1
                record_eligible = False
            if not args.stats:
                continue
        if det and det.language.code not in ["de", "en"]:
            if record_eligible:
                stats.stats["processing_stats"]["other_lang"] += 1
                record_eligible = False
            if not args.stats:
                continue
        stats.create_language_stats(det, args, description_combined)
        if record_eligible:
            stats.stats["processing_stats"]["eligible"] += 1
            classcodes_combined = list(set(classcodes + subject_classcodes)) # join and remove duplicates
            corpus_candidates[det.language.code].append((record["identifier"], description_combined, classcodes_combined, auto_classcodes))
            stats.create_corpus_stats(det.language.code, classcodes_combined, description_combined)
    if args.stats:
        stats.write_stats_file()
    if not args.corpus:
        return
    for lang, candidates in corpus_candidates.items():
        target_dir = os.path.join(CORPUS_DIR, lang)
        if not os.path.isdir(target_dir):
            os.mkdir(target_dir)
        for candidate in candidates:
            file_name = file_number + "." + candidate[0][0].replace(":", "~").replace("/", "_")
            with open(os.path.join(target_dir, file_name + ".txt") , "w") as o:
                o.write(candidate[1])
            with open(os.path.join(target_dir, file_name + ".key") , "w") as o:
                for code in candidate[2]:
                    o.write(DDC_VOCAB[code] + "\n")
            if len(candidate[3]) > 0:
                with open(os.path.join(target_dir, file_name + ".autokey") , "w") as o:
                    for code in candidate[3]:
                        o.write(DDC_VOCAB[code] + "\n")

def _load_ddc_vocab():
    global DDC_VOCAB
    with open(DDC_VOCAB_FILE, encoding="utf-8") as f:
        for line in f:
            components = line.split("\t")
            DDC_VOCAB[components[0]] = components[1].replace("\n", "")

def _cleanup_process_pool():
    global PROCESS_POOL
    still_running = []
    for process in PROCESS_POOL:
        if process.is_alive():
            still_running.append(process)
    PROCESS_POOL = still_running

def _start_new_process():
    global CONTENT_WAITING_QUEUE, PROCESS_POOL, MAX_PROCESSES
    if len(PROCESS_POOL) < MAX_PROCESSES:
        data = CONTENT_WAITING_QUEUE.pop()
        p = mp.Process(target=process_content, args=(data[0], data[1], data[2]), name="Process_" + data[1])
        PROCESS_POOL.append(p)
        p.start()
        print("started process " + str(p))
    else:
        print("Process pool currently full")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("-C", "--corpus", action="store_true", help="Create a corpus")
    parser.add_argument("-S", "--stats", action="store_true", help="Create stats files")
    parser.add_argument("-p", "--processes", type=int, help="Max number of concurrent processes")
    parser.add_argument("-s", "--start", type=int, default=0, help="ListRecords start number")
    parser.add_argument("-e", "--end", type=int, default=inf, help="ListRecords end number")
    parser.add_argument("-a", "--additional_ddc_sources", action="store_true", help="search the subject fields for additional information on record ddc classes")
    parser.add_argument("-d", "--desc_min_length", type=int, default=100, help="Minimum length of a record's description field to be eligible for the corpus (default: 100)")
    parser.add_argument("-c", "--language_min_confidence", type=float, default=95.0, help="Minimum required confidence of the polyglot language detector when identifying a record's description field language (default: 0.95)")
    parser.add_argument("-r", "--reliable_predictions_only", action="store_true", help="Only use a record for the corpus if polyglot self-reports a reliable prediction for the description field's language (stats will be generated for both cases)")
    args = parser.parse_args()
    if not (args.corpus or args.stats):
        print("Error: Either a corpus (-C) oder stats files (-S) must be created (or both)")
        sys.exit()
    if args.stats and not os.path.isdir(STATS_DIR):
        os.mkdir(STATS_DIR)
    if args.corpus and not os.path.isdir(CORPUS_DIR):
        os.mkdir(CORPUS_DIR)
    if args.processes:
        MAX_PROCESSES = args.processes

    mp.set_start_method('fork')
    _load_ddc_vocab()
    files = sorted(os.listdir(RLR_DIR))
    start_msg = ("Processing recucedListRecords with the following settings:\n" +
                 "- Create corpus: {}\n" +
                 "- Create stats: {}\n" +
                 "- Concurrent processes: {}\n" +
                 "- Start index: {}\n" +
                 "- End index: {}\n")
    print(start_msg.format(args.corpus, args.stats, MAX_PROCESSES, args.start, args.end))
    for full_name in files:
        components = full_name.split(".")
        file_number = components[1]
        file_name = components[0] + "." + components[1]
        if args.start > int(file_number) or args.end < int(file_number):
            continue
        with open(os.path.join(RLR_DIR, full_name), encoding="utf-8") as f:
            try:
                content = json.load(f)
            except json.decoder.JSONDecodeError as de:
                print(str(de))
                print(full_name)
                sys.exit()
            CONTENT_WAITING_QUEUE.append((content, file_number, args))
            _cleanup_process_pool()
            _start_new_process()
    print("All Files read, processing remaining content...")
    while len(CONTENT_WAITING_QUEUE) > 0:
        _cleanup_process_pool()
        _start_new_process()
        sleep(0.1)
    print("Waiting for all processes to finish...")
    while len(PROCESS_POOL) > 0:
        _cleanup_process_pool()
        sleep(0.1)
    print("Done!")
